{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b54490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "<!DOCTYPE html>\n",
      "    <html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:og=\"http://ogp.me/ns#\" xmlns:fb=\"https://www.facebook.com/2008/fbml\"\n",
      "        lang=\"en-US\">\n",
      "\n",
      "<head>\n",
      "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=9\" />\n",
      "    <meta charset=\"UTF-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0 user-scalable=0\" />\n",
      "    <meta property=\"fb:app_id\" content=\"702141670710132\" />\n",
      "    <meta property=\"og:type\" content=\"website\" />\n",
      "            <meta property=\"og:image:width\"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://internshala.com/internships/data-science-internship\"\n",
    "response = requests.get(url)\n",
    "\n",
    "print(response.status_code)   # should be 200 if successful\n",
    "print(response.text[:500])    # print first 500 chars of HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "639670bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs found: 51\n",
      "<div class=\"container-fluid individual_internship view_detail_button visibilityTrackerItem\" data-href=\"/internship/detail/part-time-social-media-internship-in-mumbai-at-griebs-music-pvtltd1756144471\" data-source_page=\"search_page\" employment_type=\"internship\" id=\"individual_internship_2859683\" internshipid=\"2859683\" sequential_apply_referral=\"similar_internships\">\n",
      " <div class=\"internship_meta duration_meta\">\n",
      "  <div class=\"internship-heading-container\">\n",
      "   <div class=\"company\">\n",
      "    <h3 class=\"job\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all job cards\n",
    "job_cards = soup.find_all(\"div\", class_=\"individual_internship\")\n",
    "\n",
    "print(\"Number of jobs found:\", len(job_cards))\n",
    "\n",
    "# Look at first job\n",
    "first_job = job_cards[0]\n",
    "print(first_job.prettify()[:500])   # inspect structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4b2e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Skipping job due to missing elements\n",
      "Successfully processed 0 jobs\n"
     ]
    }
   ],
   "source": [
    "jobs = []\n",
    "\n",
    "for job in job_cards:\n",
    "    try:\n",
    "        # Check if elements exist before accessing them\n",
    "        title_elem = job.find(\"h3\", class_=\"heading_4_5\")\n",
    "        company_elem = job.find(\"h4\", class_=\"heading_6\")\n",
    "        link_elem = job.find(\"a\")\n",
    "        \n",
    "        if title_elem and company_elem and link_elem:\n",
    "            title = title_elem.get_text(strip=True)\n",
    "            company = company_elem.get_text(strip=True)\n",
    "            link = \"https://internshala.com\" + link_elem[\"href\"]\n",
    "\n",
    "            jobs.append({\n",
    "                \"title\": title,\n",
    "                \"company\": company,\n",
    "                \"url\": link\n",
    "            })\n",
    "        else:\n",
    "            print(\"Skipping job due to missing elements\")\n",
    "            continue\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing job: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Successfully processed {len(jobs)} jobs\")\n",
    "for j in jobs[:5]:   # show first 5 jobs\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6f31585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUGGING HTML STRUCTURE ===\n",
      "\n",
      "All h3 elements:\n",
      "  Class: ['job-internship-name'], Text: Social Media...\n",
      "\n",
      "All h4 elements:\n",
      "\n",
      "All links:\n",
      "  href: /internship/detail/part-time-social-media-internsh...\n"
     ]
    }
   ],
   "source": [
    "# Debug: Let's examine the actual HTML structure of the first job card\n",
    "print(\"=== DEBUGGING HTML STRUCTURE ===\")\n",
    "first_job = job_cards[0]\n",
    "\n",
    "# Find all h3 and h4 elements to see what classes are available\n",
    "print(\"\\nAll h3 elements:\")\n",
    "for h3 in first_job.find_all(\"h3\"):\n",
    "    print(f\"  Class: {h3.get('class')}, Text: {h3.get_text(strip=True)[:50]}...\")\n",
    "\n",
    "print(\"\\nAll h4 elements:\")\n",
    "for h4 in first_job.find_all(\"h4\"):\n",
    "    print(f\"  Class: {h4.get('class')}, Text: {h4.get_text(strip=True)[:50]}...\")\n",
    "\n",
    "print(\"\\nAll links:\")\n",
    "for link in first_job.find_all(\"a\"):\n",
    "    if link.get(\"href\"):\n",
    "        print(f\"  href: {link.get('href')[:50]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6dde176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 jobs\n",
      "{'title': 'Social Media', 'company': 'Social MediaGriebs Music Private LimitedActively hiring', 'url': 'https://internshala.com/internship/detail/part-time-social-media-internship-in-mumbai-at-griebs-music-pvtltd1756144471'}\n",
      "{'title': 'Market Research ( female only) - field work', 'company': 'Market Research ( female only) - field workVital  SynergieActively hiring', 'url': 'https://internshala.com/internship/detail/market-research-internship-in-delhi-at-vital-synergie1758521055'}\n",
      "{'title': 'Motion Graphics (AI Tools)', 'company': 'Motion Graphics (AI Tools)Integral SolutionActively hiring', 'url': 'https://internshala.com/internship/detail/motion-graphics-ai-tools-internship-in-bangalore-at-integral-solution1757494564'}\n"
     ]
    }
   ],
   "source": [
    "def scrape_internshala(job_title):\n",
    "    query = job_title.replace(\" \", \"-\")\n",
    "    url = f\"https://internshala.com/internships/{query}-internship\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    jobs = []\n",
    "    for job in soup.find_all(\"div\", class_=\"individual_internship\"):\n",
    "        try:\n",
    "            # Use the correct class names based on debug output\n",
    "            title_elem = job.find(\"h3\", class_=\"job-internship-name\")\n",
    "            link_elem = job.find(\"a\")\n",
    "            \n",
    "            if title_elem and link_elem:\n",
    "                title = title_elem.get_text(strip=True)\n",
    "                link = \"https://internshala.com\" + link_elem[\"href\"]\n",
    "                \n",
    "                # Try to find company name - might be in different elements\n",
    "                company_elem = job.find(\"h4\") or job.find(\"div\", class_=\"company\") or job.find(\"a\", class_=\"company\")\n",
    "                company = company_elem.get_text(strip=True) if company_elem else \"Company not found\"\n",
    "                \n",
    "                jobs.append({\"title\": title, \"company\": company, \"url\": link})\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing job: {e}\")\n",
    "            continue\n",
    "            \n",
    "    return jobs\n",
    "\n",
    "# Test the function\n",
    "jobs = scrape_internshala(\"data science\")\n",
    "print(f\"Found {len(jobs)} jobs\")\n",
    "for job in jobs[:3]:  # Show first 3 jobs\n",
    "    print(job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cec5f089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1: https://internshala.com/internships/data-science-internship\n",
      "Found 51 jobs on page 1\n",
      "Scraping page 2: https://internshala.com/internships/data-science-internship?page=2\n",
      "Found 51 jobs on page 2\n",
      "\n",
      "Total jobs found: 102\n",
      "\n",
      "Job 1:\n",
      "  Title: Social Media\n",
      "  Company: Social MediaGriebs Music Private LimitedActively hiring\n",
      "  Location: Location not specified\n",
      "  Duration: Duration not specified\n",
      "  URL: https://internshala.com/internship/detail/part-time-social-media-internship-in-mumbai-at-griebs-music-pvtltd1756144471\n",
      "\n",
      "Job 2:\n",
      "  Title: Market Research ( female only) - field work\n",
      "  Company: Market Research ( female only) - field workVital  SynergieActively hiring\n",
      "  Location: Location not specified\n",
      "  Duration: Duration not specified\n",
      "  URL: https://internshala.com/internship/detail/market-research-internship-in-delhi-at-vital-synergie1758521055\n",
      "\n",
      "Job 3:\n",
      "  Title: Motion Graphics (AI Tools)\n",
      "  Company: Motion Graphics (AI Tools)Integral SolutionActively hiring\n",
      "  Location: Location not specified\n",
      "  Duration: Duration not specified\n",
      "  URL: https://internshala.com/internship/detail/motion-graphics-ai-tools-internship-in-bangalore-at-integral-solution1757494564\n",
      "\n",
      "Job 4:\n",
      "  Title: Data Analytics- Internship\n",
      "  Company: Data Analytics- InternshipBrooke Hospital For Animals (India)Actively hiring\n",
      "  Location: Location not specified\n",
      "  Duration: Duration not specified\n",
      "  URL: https://internshala.com/internship/detail/data-analytics-internship-internship-in-noida-at-brooke-hospital-for-animals-india1758870452\n",
      "\n",
      "Job 5:\n",
      "  Title: AI Agent Development\n",
      "  Company: AI Agent DevelopmentHEU Technologies Private LimitedActively hiring\n",
      "  Location: Location not specified\n",
      "  Duration: Duration not specified\n",
      "  URL: https://internshala.com/internship/detail/ai-agent-development-internship-in-kochi-at-heu-technologies-private-limited1758613747\n"
     ]
    }
   ],
   "source": [
    "# Enhanced scraper with better company detection and data export\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def scrape_internshala_enhanced(job_title, max_pages=3):\n",
    "    \"\"\"\n",
    "    Enhanced scraper that tries multiple approaches to find company names\n",
    "    and can handle multiple pages\n",
    "    \"\"\"\n",
    "    all_jobs = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        query = job_title.replace(\" \", \"-\")\n",
    "        url = f\"https://internshala.com/internships/{query}-internship\"\n",
    "        if page > 1:\n",
    "            url += f\"?page={page}\"\n",
    "            \n",
    "        print(f\"Scraping page {page}: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        job_cards = soup.find_all(\"div\", class_=\"individual_internship\")\n",
    "        print(f\"Found {len(job_cards)} jobs on page {page}\")\n",
    "        \n",
    "        for job in job_cards:\n",
    "            try:\n",
    "                # Get job title\n",
    "                title_elem = job.find(\"h3\", class_=\"job-internship-name\")\n",
    "                title = title_elem.get_text(strip=True) if title_elem else \"Title not found\"\n",
    "                \n",
    "                # Get job link\n",
    "                link_elem = job.find(\"a\")\n",
    "                link = \"https://internshala.com\" + link_elem[\"href\"] if link_elem else \"Link not found\"\n",
    "                \n",
    "                # Try multiple approaches to find company name\n",
    "                company = \"Company not found\"\n",
    "                company_selectors = [\n",
    "                    \"h4\",\n",
    "                    \"div.company\",\n",
    "                    \"a.company\", \n",
    "                    \"span.company-name\",\n",
    "                    \".company_name\",\n",
    "                    \"[class*='company']\"\n",
    "                ]\n",
    "                \n",
    "                for selector in company_selectors:\n",
    "                    company_elem = job.select_one(selector)\n",
    "                    if company_elem:\n",
    "                        company = company_elem.get_text(strip=True)\n",
    "                        if company and company != title:  # Make sure it's not the same as title\n",
    "                            break\n",
    "                \n",
    "                # Extract additional info if available\n",
    "                location = \"Location not specified\"\n",
    "                duration = \"Duration not specified\"\n",
    "                \n",
    "                # Try to find location\n",
    "                location_elem = job.find(\"span\", class_=\"location\") or job.find(\"div\", class_=\"location\")\n",
    "                if location_elem:\n",
    "                    location = location_elem.get_text(strip=True)\n",
    "                \n",
    "                # Try to find duration\n",
    "                duration_elem = job.find(\"span\", class_=\"duration\") or job.find(\"div\", class_=\"duration\")\n",
    "                if duration_elem:\n",
    "                    duration = duration_elem.get_text(strip=True)\n",
    "                \n",
    "                all_jobs.append({\n",
    "                    \"title\": title,\n",
    "                    \"company\": company,\n",
    "                    \"location\": location,\n",
    "                    \"duration\": duration,\n",
    "                    \"url\": link\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing job: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return all_jobs\n",
    "\n",
    "# Test the enhanced scraper\n",
    "jobs = scrape_internshala_enhanced(\"data science\", max_pages=2)\n",
    "print(f\"\\nTotal jobs found: {len(jobs)}\")\n",
    "\n",
    "# Display first few jobs\n",
    "for i, job in enumerate(jobs[:5]):\n",
    "    print(f\"\\nJob {i+1}:\")\n",
    "    print(f\"  Title: {job['title']}\")\n",
    "    print(f\"  Company: {job['company']}\")\n",
    "    print(f\"  Location: {job['location']}\")\n",
    "    print(f\"  Duration: {job['duration']}\")\n",
    "    print(f\"  URL: {job['url']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be2cbb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== JOB ANALYSIS ===\n",
      "Total jobs scraped: 102\n",
      "Unique companies: 51\n",
      "Jobs with company info: 102\n",
      "\n",
      "=== TOP COMPANIES ===\n",
      "company\n",
      "Social MediaGriebs Music Private LimitedActively hiring                    2\n",
      "Creative DesignVijanX                                                      2\n",
      "AI Researcher (Speech & Audio)Josh Talks                                   2\n",
      "Social Media Manager & Content StrategistDuostones LLC                     2\n",
      "Data AnalyticsEvoNexisActively hiring                                      2\n",
      "Machine LearningEvoNexisActively hiring                                    2\n",
      "Financial AnalystZean Lithos And Company Private LimitedActively hiring    2\n",
      "Business AnalyticsEvoNexisActively hiring                                  2\n",
      "AI Agent AutomationCalyco Paints                                           2\n",
      "Data Science (Subject Matter Expert)MetaSphere Ventures                    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== SAMPLE DATA ===\n",
      "                                         title  \\\n",
      "0                                 Social Media   \n",
      "1  Market Research ( female only) - field work   \n",
      "2                   Motion Graphics (AI Tools)   \n",
      "3                   Data Analytics- Internship   \n",
      "4                         AI Agent Development   \n",
      "\n",
      "                                             company                location  \\\n",
      "0  Social MediaGriebs Music Private LimitedActive...  Location not specified   \n",
      "1  Market Research ( female only) - field workVit...  Location not specified   \n",
      "2  Motion Graphics (AI Tools)Integral SolutionAct...  Location not specified   \n",
      "3  Data Analytics- InternshipBrooke Hospital For ...  Location not specified   \n",
      "4  AI Agent DevelopmentHEU Technologies Private L...  Location not specified   \n",
      "\n",
      "                 duration                                                url  \n",
      "0  Duration not specified  https://internshala.com/internship/detail/part...  \n",
      "1  Duration not specified  https://internshala.com/internship/detail/mark...  \n",
      "2  Duration not specified  https://internshala.com/internship/detail/moti...  \n",
      "3  Duration not specified  https://internshala.com/internship/detail/data...  \n",
      "4  Duration not specified  https://internshala.com/internship/detail/ai-a...  \n",
      "\n",
      "Data saved to:\n",
      "- internshala_jobs.csv\n",
      "- internshala_jobs.json\n"
     ]
    }
   ],
   "source": [
    "# Data Export and Analysis\n",
    "if jobs:\n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df = pd.DataFrame(jobs)\n",
    "    \n",
    "    print(\"=== JOB ANALYSIS ===\")\n",
    "    print(f\"Total jobs scraped: {len(df)}\")\n",
    "    print(f\"Unique companies: {df['company'].nunique()}\")\n",
    "    print(f\"Jobs with company info: {len(df[df['company'] != 'Company not found'])}\")\n",
    "    \n",
    "    print(\"\\n=== TOP COMPANIES ===\")\n",
    "    top_companies = df['company'].value_counts().head(10)\n",
    "    print(top_companies)\n",
    "    \n",
    "    print(\"\\n=== SAMPLE DATA ===\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Save to files\n",
    "    df.to_csv('internshala_jobs.csv', index=False)\n",
    "    df.to_json('internshala_jobs.json', orient='records', indent=2)\n",
    "    \n",
    "    print(f\"\\nData saved to:\")\n",
    "    print(\"- internshala_jobs.csv\")\n",
    "    print(\"- internshala_jobs.json\")\n",
    "else:\n",
    "    print(\"No jobs found to analyze\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ecaee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import json\n",
    "\n",
    "# Initialize Ollama with the correct model name\n",
    "ollama = Ollama(model=\"llama3:8b\")\n",
    "\n",
    "def clean_job_with_llm(job):\n",
    "    \"\"\"\n",
    "    Use Llama 2 via Ollama to clean and standardize job information\n",
    "    \"\"\"\n",
    "    raw_text = f\"\"\"\n",
    "    Title: {job['title']}\n",
    "    Company: {job['company']}\n",
    "    Location: {job['location']}\n",
    "    Duration: {job['duration']}\n",
    "    URL: {job['url']}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Clean and extract the following job info.\n",
    "    - Remove extra text like 'Actively hiring'\n",
    "    - Fix formatting\n",
    "    - Return ONLY valid JSON with fields:\n",
    "      title, company, location, duration, url\n",
    "\n",
    "    Job info:\n",
    "    {raw_text}\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.invoke(prompt)\n",
    "\n",
    "    try:\n",
    "        cleaned = json.loads(response)\n",
    "        return cleaned\n",
    "    except Exception:\n",
    "        return {\n",
    "            \"error\": \"Could not parse JSON\",\n",
    "            \"raw\": response\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d36eb808",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m cleaned_jobs = []\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m jobs:  \u001b[38;5;66;03m# jobs from your scrape_internshala_enhanced\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     cleaned = \u001b[43mclean_job_with_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     cleaned_jobs.append(cleaned)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Save as CSV\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mclean_job_with_llm\u001b[39m\u001b[34m(job)\u001b[39m\n\u001b[32m     11\u001b[39m raw_text = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[33mTitle: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33mCompany: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob[\u001b[33m'\u001b[39m\u001b[33mcompany\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m \u001b[33mURL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob[\u001b[33m'\u001b[39m\u001b[33murl\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     19\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[33mClean and extract the following job info.\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[33m- Remove extra text like \u001b[39m\u001b[33m'\u001b[39m\u001b[33mActively hiring\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m \u001b[33m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_text\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m response = \u001b[43mollama\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     33\u001b[39m     cleaned = json.loads(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\desha\\.conda\\envs\\langchain_env\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:389\u001b[39m, in \u001b[36mBaseLLM.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    380\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    385\u001b[39m     **kwargs: Any,\n\u001b[32m    386\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    387\u001b[39m     config = ensure_config(config)\n\u001b[32m    388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m         .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    400\u001b[39m         .text\n\u001b[32m    401\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\desha\\.conda\\envs\\langchain_env\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:766\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    759\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    763\u001b[39m     **kwargs: Any,\n\u001b[32m    764\u001b[39m ) -> LLMResult:\n\u001b[32m    765\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\desha\\.conda\\envs\\langchain_env\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:971\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    957\u001b[39m     run_managers = [\n\u001b[32m    958\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    959\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    969\u001b[39m         )\n\u001b[32m    970\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m    979\u001b[39m     run_managers = [\n\u001b[32m    980\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m    981\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    988\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m    989\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\desha\\.conda\\envs\\langchain_env\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:792\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    782\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    783\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    788\u001b[39m     **kwargs: Any,\n\u001b[32m    789\u001b[39m ) -> LLMResult:\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    791\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m792\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    796\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    799\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    800\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    801\u001b[39m         )\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    803\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\desha\\.conda\\envs\\langchain_env\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:437\u001b[39m, in \u001b[36mOllama._generate\u001b[39m\u001b[34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[39m\n\u001b[32m    435\u001b[39m generations = []\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     final_chunk = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m     generations.append([final_chunk])\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations=generations)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\desha\\.conda\\envs\\langchain_env\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:349\u001b[39m, in \u001b[36m_OllamaCommon._stream_with_aggregation\u001b[39m\u001b[34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_stream_with_aggregation\u001b[39m(\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    342\u001b[39m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    346\u001b[39m     **kwargs: Any,\n\u001b[32m    347\u001b[39m ) -> GenerationChunk:\n\u001b[32m    348\u001b[39m     final_chunk: Optional[GenerationChunk] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_generate_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stream_response_to_generation_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\desha\\.conda\\envs\\langchain_env\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:194\u001b[39m, in \u001b[36m_OllamaCommon._create_generate_stream\u001b[39m\u001b[34m(self, prompt, stop, images, **kwargs)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_generate_stream\u001b[39m(\n\u001b[32m    187\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    188\u001b[39m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    191\u001b[39m     **kwargs: Any,\n\u001b[32m    192\u001b[39m ) -> Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    193\u001b[39m     payload = {\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt, \u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m: images}\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_stream(\n\u001b[32m    195\u001b[39m         payload=payload,\n\u001b[32m    196\u001b[39m         stop=stop,\n\u001b[32m    197\u001b[39m         api_url=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.base_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/api/generate\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    198\u001b[39m         **kwargs,\n\u001b[32m    199\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\desha\\.conda\\envs\\langchain_env\\Lib\\site-packages\\requests\\models.py:869\u001b[39m, in \u001b[36mResponse.iter_lines\u001b[39m\u001b[34m(self, chunk_size, decode_unicode, delimiter)\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[32m    861\u001b[39m \u001b[33;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[32m    862\u001b[39m \u001b[33;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[32m    863\u001b[39m \n\u001b[32m    864\u001b[39m \u001b[33;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[32m    865\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    867\u001b[39m pending = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_unicode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_unicode\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\desha\\.conda\\envs\\langchain_env\\Lib\\site-packages\\requests\\utils.py:562\u001b[39m, in \u001b[36mstream_decode_response_unicode\u001b[39m\u001b[34m(iterator, r)\u001b[39m\n\u001b[32m    559\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    561\u001b[39m decoder = codecs.getincrementaldecoder(r.encoding)(errors=\u001b[33m\"\u001b[39m\u001b[33mreplace\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrv\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrv\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\desha\\.conda\\envs\\langchain_env\\Lib\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\desha\\.conda\\envs\\langchain_env\\Lib\\site-packages\\urllib3\\response.py:1088\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1072\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\desha\\.conda\\envs\\langchain_env\\Lib\\site-packages\\urllib3\\response.py:1248\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1245\u001b[39m     amt = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1250\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\desha\\.conda\\envs\\langchain_env\\Lib\\site-packages\\urllib3\\response.py:1167\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1168\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\desha\\.conda\\envs\\langchain_env\\Lib\\socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "cleaned_jobs = []\n",
    "\n",
    "for job in jobs:  # jobs from your scrape_internshala_enhanced\n",
    "    cleaned = clean_job_with_llm(job)\n",
    "    cleaned_jobs.append(cleaned)\n",
    "\n",
    "# Save as CSV\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([j for j in cleaned_jobs if isinstance(j, dict) and \"error\" not in j])\n",
    "df.to_csv(\"cleaned_jobs.csv\", index=False)\n",
    "\n",
    "print(\" Cleaned job data saved.\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703481ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
